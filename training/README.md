# ü§ñ CryptoAI Vision Transformer Training

## Overview

Cette phase impl√©mente l'entra√Ænement d'un Vision Transformer pour la reconnaissance de patterns techniques dans les graphiques crypto. Le syst√®me g√©n√®re automatiquement un dataset de 1000+ images synth√©tiques et entra√Æne un mod√®le ViT avec une pr√©cision cible de 75-85%.

## üéØ Pattern Classes

| ID | Pattern | Description |
|----|---------|-------------|
| 0 | Head and Shoulders | Pattern de retournement baissier classique |
| 1 | Double Top | Confirmation de r√©sistance et retournement |
| 2 | Double Bottom | Confirmation de support et rebond |
| 3 | Ascending Triangle | Consolidation avec breakout haussier |
| 4 | Descending Triangle | Consolidation avec breakout baissier |
| 5 | Cup and Handle | Pattern de continuation haussi√®re |
| 6 | Bullish Flag | Correction temporaire en tendance haussi√®re |
| 7 | Bearish Flag | Correction temporaire en tendance baissi√®re |
| 8 | Support/Resistance | Niveaux techniques de rebond |
| 9 | Breakout | Sortie de phase de consolidation |

## üöÄ Quick Start (Google Colab)

### M√©thode 1: Notebook Colab (Recommand√©)
1. Ouvrir Google Colab
2. Upload `CryptoAI_Vision_Training.ipynb`
3. Runtime ‚Üí Change runtime type ‚Üí GPU
4. Upload `train_vision_transformer.py`
5. Ex√©cuter toutes les cellules

### M√©thode 2: Script Direct
1. Upload `train_vision_transformer.py` dans Colab
2. Ex√©cuter: `!python train_vision_transformer.py`

## üíª Local Training

```bash
# Installation des d√©pendances
pip install -r requirements.txt

# Lancement de l'entra√Ænement
python train_vision_transformer.py
```

**‚ö†Ô∏è Attention**: L'entra√Ænement local n√©cessite un GPU avec au moins 4GB VRAM.

## üìä Architecture du Mod√®le

```
Vision Transformer (ViT-Base)
‚îú‚îÄ‚îÄ Patch Embedding (16x16 patches)
‚îú‚îÄ‚îÄ Transformer Encoder (12 layers)
‚îú‚îÄ‚îÄ Classification Head (10 classes)
‚îî‚îÄ‚îÄ Output: Pattern probabilities
```

### Caract√©ristiques:
- **Base Model**: `google/vit-base-patch16-224-in21k`
- **Input Size**: 224x224 RGB
- **Quantification**: INT8 pour optimisation Lambda
- **Model Size**: ~50MB (quantifi√©)

## üîß Configuration

```python
CONFIG = {
    'image_size': (224, 224),
    'num_classes': 10,
    'batch_size': 32,
    'num_epochs': 15,
    'learning_rate': 1e-4,
    'dataset_size': 1000,
    'model_name': 'google/vit-base-patch16-224-in21k'
}
```

## üìà Dataset Generation

Le g√©n√©rateur cr√©e automatiquement des patterns synth√©tiques avec:
- **R√©alisme**: Bruit de prix, volumes, grilles
- **Variabilit√©**: Rotations, couleurs, √©chelles
- **Labeling**: Classification automatique
- **Qualit√©**: Charts haute r√©solution (224x224)

### Exemple de g√©n√©ration:

```python
generator = ChartPatternGenerator()
head_shoulders = generator.generate_head_and_shoulders()
double_top = generator.generate_double_top()
```

## üß™ Evaluation

### M√©triques:
- **Accuracy**: % de pr√©dictions correctes
- **Validation Split**: 80% train / 20% validation
- **Target**: 75-85% accuracy
- **Visualization**: Sample predictions avec ground truth

### Test du mod√®le ONNX:
```python
import onnxruntime

ort_session = onnxruntime.InferenceSession('crypto_pattern_model_quantized.onnx')
predictions = ort_session.run(None, {'input': test_image})
```

## üìÅ Output Files

Apr√®s entra√Ænement:
```
training/
‚îú‚îÄ‚îÄ best_model.pth                     # PyTorch model (full)
‚îú‚îÄ‚îÄ crypto_pattern_model.onnx          # ONNX model (full)
‚îú‚îÄ‚îÄ crypto_pattern_model_quantized.onnx # ONNX quantized (production)
‚îî‚îÄ‚îÄ predictions_visualization.png      # Sample predictions
```

## üîÑ Training Pipeline

1. **Dataset Generation** (5 min)
   - 1000 images synth√©tiques
   - 10 classes de patterns
   - Augmentation de donn√©es

2. **Model Loading** (1 min)
   - Vision Transformer pr√©-entra√Æn√©
   - Fine-tuning du classifier

3. **Training Loop** (15-30 min sur GPU)
   - 15 epochs avec early stopping
   - Validation √† chaque epoch
   - Sauvegarde du meilleur mod√®le

4. **Export & Quantization** (2 min)
   - Export ONNX standard
   - Quantification INT8
   - Validation du mod√®le

## üí° Tips & Optimizations

### Pour am√©liorer la pr√©cision:
- Augmenter `num_epochs` √† 25-30
- R√©duire `learning_rate` √† 5e-5
- Augmenter `dataset_size` √† 2000+
- Ajouter plus d'augmentations

### Pour r√©duire la taille du mod√®le:
- Utiliser ViT-Small: `google/vit-small-patch16-224`
- Quantification plus agressive
- Pruning des couches

## üöÄ Next Phase

Une fois l'entra√Ænement termin√© avec succ√®s (>75% accuracy):

**Phase 3: Backend Integration**
- Deploy mod√®le ONNX dans Lambda Container
- API endpoints pour inf√©rence en temps r√©el
- Tests end-to-end avec donn√©es crypto r√©elles

## üìã Checklist Phase 2

- [x] Script d'entra√Ænement cr√©√©
- [x] Notebook Colab pr√©par√©  
- [x] G√©n√©rateur de patterns impl√©ment√©
- [ ] **Ex√©cuter l'entra√Ænement sur GPU**
- [ ] **Validation de la pr√©cision (>75%)**
- [ ] **Export ONNX quantifi√©**
- [ ] **T√©l√©chargement des mod√®les**

---

**‚è±Ô∏è Temps estim√©**: 45-60 minutes sur Google Colab (GPU gratuit)  
**üí∞ Co√ªt**: 0‚Ç¨ (Colab gratuit) ou ~5‚Ç¨ (Colab Pro pour GPU plus rapide)